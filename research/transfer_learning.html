<!DOCTYPE html>
<html lang="en">
<head>
  <title>Transfer Learning and Multi-Task Learning</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link rel="stylesheet" href="style/research.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</head>

<body>

  <div class="header">
    <h2>Research Interests<</h2>
  </div>

<div class="topnav">
  <a href="research_interests/capsule_networks.html">Capsule Networks</a>
  <a href="research_interests/adversarial_networks.html">Adversarial Networks</a>
  <a href="research_interests/reinforcement_learning.html">Reinforcement Learning</a>
  <a href="research_interests/graph_modelling.html">Graph Structure Modelling</a>
  <a href="research_interests/horizontal_learning.html">Transfer Learning</a>
  <a href="research_interests/search.html">Search</a>
  <a href="research_interests/other.html" style="float:right">Other</a>
  
</div>

  <h2>Transfer, Multi-task &amp; Domain Adaptation Learning</h2>
  More often than not in machine learning (ML), we train models from scratch and do not take advantage of data and learnt models for similar and related tasks. More recently, the goal of consolidating knowledge across tasks has become a very active area of research (transfer learning or meta-learning). I refer to this type of learning as ‘horizontal learning’ (learning what to transfer across tasks) as opposed to ‘vertical learning’ (task specific learning).

  An interesting recent paper by Andrychowicz et al. (andrychowicz2016learning) view transfer learning as an optimization problem, proposing optimization as a learning problem and comparing against typical optimizations such as ADAM, Nesterov Accelerated Gradient (NAG) and RMSProp. This allows the optimizers to be identified that are well suited to particular types of functions.

  We witnessed a remarkable degree of transfer, with for example theLSTM optimizer trained on 12,288 parameter neural art tasks being able to generalize to tasks with 49,152 parameters, different styles, and different content images all at the same time. We observedsimilar impressive results when transferring to different architectures in the MNIST task.The results on the CIFAR image labeling task show that the LSTM optimizers outperform hand-engineered optimizers when transferring to datasets drawn from the same data distribution.
  <h2>One/Few Shot Learning</h2>
  Humans often learn from only few examples, leveraging knowledge from what they know about related concepts in the world to help guide their understanding for a new concept. I am interested in tasks where little is known about a given class or at least classes are highly imbalanced, a ubiquity in ML.

  One shot imitation learning is a recent approach presented by Duan et al. ~\cite{duan2017one} to learn from little number of examples using an imitation network

  Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations. For example,a task could be to stack all blocks on a table in to a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a sub-set of all tasks. A neural net is trained that takes as input one demonstration and the current state(which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention al-lows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an over-whelming variety of tasks
  <h2>Deep Reinforcement Learning in Natural Language State Space</h2>
  Recently, I have taken a strong interest and appreciation for decision process, control and prediction for goal-orientated agent based modeling. Hence, I have recently been considering where NLP problems can be viewed as agents navigating a language state space. Commonly, dialogue systems usually have a goal of coming to some significant resolve which can be viewed as a goal, likewise in Question-Answering system the goal is to produce the correct, or most contextually correct answer to the user.
  <h2>Generative Modeling for Text Generation</h2>
  For classification, discriminative modeling involves direct posterior estimation, the conditional probability $latex p(y|x)$. In generative modeling the joint distrbution $latex p(x,y)$ is first estimated and then prediction is carried out using Bayes rule. In this sense, generative models are not only used for prediction but also for interpolation of likely sample pairs $latex (x,y)$ from the estimated posterior.
  Ng and Jordan ~\cite{ng2002discriminative} in 2002 discuss this in their paper \textit{On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes}. They tested the hypothesis that discriminative models such as logistic regression perform better as the datasets size grows, but for initial performance on smaller batches, generative models such as Naive Bayes are favored.
  When the task involves direct estimation of $latex p(x,y)$ it makes sense to directly estimate the underlying distribution using a generative model, given that are enough instances to train on, proportional to the number of parameters in the generative model. Furthermore, generative models have the benefit of interpolating to unseen instances in $latex \mathcal{D}$ e.g generating whole new sentences in continuous space and not being bounded by the vocabulary only existing in $latex \mathcal{D}$.
  Bowman et al ~\cite{bowman2015generating} generated sentences from a continuous space using an RNN variational autoencoder language model, essentially generating whole sentence instead of word by word predictions. This true type of generative modeling better captures higher level syntactic attributes of the sentences and underlying joint distribution. Typically, coherency in new generated sentences is difficult. That is to say, what type of constraints need to be used for generative models to produce new sentences that are highly coherent but yet unique in their interpolation. This is a potential research question I would be interested in pursuing.

  Additionally, generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) have seen great success recently, primarily on vision tasks. Although, only very recently have generative models made an impact on NLP text generation. Hu et al.~\cite{hu2017toward} have proposed to use VAEs that use the wake sleep algorithm to generate fake samples of short sentences during the sleep phase that are then fed back in the training process during the wake phase. The model is based off ~\cite{bowman2015generating}

  \textbf{Applying discriminators to text generation is hard due to the non-differentiability of discrete samples (Yu et al., 2017; Zhang et al., 2016; Kusner \&amp; Hernndez-Lobato, 2016). Bowman et al. (2015); Tang et al. (2016); Yang et al. (2017) instead use VAEs without discriminators. All these text generation methods do not learn disentangled latent representations, resulting in randomized and uncontrollable samples. In contrast, disentangled
  generation in visual domain has made impressive progress. E.g., InfoGAN (Chen et al., 2016), which resembles the extended sleep procedure of our joint VAE/wakesleep algorithm, disentangles latent representation in an unsupervised manner. The semantic of each dimension is observed after training rather than designated by users in a controlled way. Siddharth et al. (2017); Kingma et al.
  (2014) base on VAEs and obtain disentangled image representations with semi-supervised learning. Zhou \&amp; NeuToward Controlled Generation of Text big (2017) extend semi-supervised VAEs for text transduction. In contrast, our model combines VAEs with discriminators which provide a better, holistic metric compared to element-wise reconstruction. Moreover, most of these approaches have only focused on the disentanglement of the structured part of latent representations, while ignoring potential dependence of the structured code with attributes not explicitly encoded. We address this by introducing an independency constraint, and show its effectiveness for improved interpretability.}

  Also, GANs in particular are notoriously difficult to train, which arises the question, <em>What controls should be used to ensure stability and convergence when training GANs for in continuous embedding space</em> $latex \mathcal{X}$ ?. Hu et al.'s paper has made a start on answering this for text generation with VAEs but this is only the beginning of this kind of work where many improvements are still to be made to generate human level novel coherent sentences.

  &nbsp;

</body>


</html>
