<!DOCTYPE html>
<html lang="en">
<head>
  <title>Bootstrap Example</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link rel="stylesheet" href="style/research.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</head>

<body>
  
  <div class="header">
  <h2>Research Interests<</h2>
</div>

<div class="topnav">
  <a href="research/capsule_networks.html">Capsule Networks</a>
  <a href="research/adversarial_networks.html">Adversarial Networks</a>
  <a href="research/reinforcement_learning.html">Reinforcement Learning</a>
  <a href="research/graph_modelling.html">Graph Structure Modelling</a>
  <a href="researc/horizontal_learning.html">Transfer Learning</a>
  <a href="research/search.html">Search</a>
  <a href="research/other.html" style="float:right">Other</a>
  
</div>
 
<div class="row">
  <div class="leftcolumn">
    <div class="card">
      <h2>Capsule Networks</h2>
      <h5>Here we discuss how Capsule Networks work and how they are different to Convolutional Neural Networks through the use 
        of a dynamic routing algorithm, Dec 28, 2017</h5>
      <div class="fakeimg" style="height:100px max-height:100%; max-width:100%;"><a href="images/capsule_networks.png"></div>
      <p>This page discusses some of my recent research ventures/interests.</p>
    </div>
    <div class="card">
      <h2>Adversarial Networks</h2>
      <h5>We will discuss basic Generative Adversarial Networks to some variations such as WassersteinGANs, Conditional GANs Bayesian GANs etc., Sep 2, 2017</h5>
      <div class="fakeimg" style="height:100px max-height:100%; max-width:100%;"><a href="images/GAN.png"></div>
      <p>Some text..</p>
    </div>
  </div>
  <div class="rightcolumn">
    <div class="card">
      <h2>About Me</h2>
      <div class="fakeimg" style="height:100px;">Image</div>
      <p>Some text about me in culpa qui officia deserunt mollit anim..</p>
    </div>
    <div class="card">
      <h3>Popular Post</h3>
      <div class="fakeimg"><p>Image</p></div>
      <div class="fakeimg"><p>Image</p></div>
      <div class="fakeimg"><p>Image</p></div>
    </div>
    <div class="card">
      <h3>Follow Me</h3>
      <p>Some text..</p>
    </div>
  </div>
</div>

<div class="footer">
  <h2>Footer</h2>
</div>
  
  
  
  <h1>Research Interests</h1>
  My current and primary research interests can be split into 5 broad categories; Sequence Modeling, Graph/Tree Modeling, Transfer Learning, Reinforcement Learning and Generative Modeling. Non-surprisingly, these categories overlap considerably e.g knowledge transfer is concerned with all other sections, while many discussed issues in sequence modeling are also present in Graph and Tree structure models.
  <h2>Deep Sequence Modeling</h2>
  Sequence modeling is a major part of many tasks in AI such as object/video recognition, speech recognition, NLP classification tasks, agent based modeling and essentially any tasks that are sequential in nature. Below contains major research facets in this field and how I wish to contribute to them to extend the existing work in each respective area.
  \subsubsection{Sequence to Sequence Modeling in NLP}
  Sequence to sequence models in NLP is commonly used for machine translation, multi-modal image captioning, question-answering systems, conversational systems, text summarization etc. Many of these tasks are still need of algorithmic improvements although some recent advances have made a distinct improvement ~\cite{bahdanau2014neural,sutskever2014sequence,bahdanau2014neural,xu2015show}
  <h3>Pairwise Learning</h3>
  Learning relationships between sequences is an important focus within deep learning, particularly in NLP. Siamese Networks ~\cite{bromley1994signature} are used to learn relationships between instances e.g sentences, images etc. Many problems require relationship learning of this type but it has not been applied extensively in fields where it can be of crucial benefit, such as NLP (with the exception of tasks such as sentence similarity). I foresee pairwise learning architectures to play a significant role for learning relationships for instances within a given dataset but also learning relationships across different datasets where the instances arise from different but related spaces $latex \mathcal{X}_{t1}$ and $latex \mathcal{X}_{t2}$. The design of appropriate optimization and loss functions that are designed specifically for relationship learning has not been explored in great detail and identifying appropriate loss functions for types of tasks (e.g contrastive loss vs mse) be a worthwhile research venture. Tangential comparisons to neuroscience research that describes how particular clusters of neurons activate based EEG and fMRI studies could potentially align, motivate and partially justify the ideas for relationship learning based on how the our brain carries out learning of related concepts.
  <h3>External Memory, Pointer Networks and Neural Turing Machines</h3>
  <strong>External memory</strong> has become an interest of mine, decoupling memory from the neural network while being fully differentiable had not been achieved up until recently. Graves (graves2014neural) introduced<strong> Neural Turing Machines</strong> (NTMs) in 2014 that achieved this, meaning an end to end memory network was designed with read and write operations to a memory matrix that attends to all memory elements. Vinyals et al. <strong>vinyals2016matching</strong> have used external memory to help with the problem of learning from new concepts with few instance, often referred to as few shot-learning.

  <strong>Pointer networks</strong> were introduced by Vinyals et al. (vinyals2015pointer) for sequence modelling tasks that require to keep track of ordering in the input for the task of ordering variable sized sequences. The idea here is to learn the probability of an output sequence of discrete tokens that correspond to positions in an input sequence, as standard recurrent networks have a difficult time keeping input order into account, a problem even the aforementioned NTMs cannot overcome. Pointer networks are essentially a variation of <em>Seq-2-Seq</em> models with attention, where attention is a softmax distribution with size equal the input length. In comparison to encoder decoder architectures where we only look back at the last encoder state whereas the pointer network can attend to different states of the encoder. Essentially, in comparison to using attention to output a weighted average of encoder states for the decoder state utilize, Vinyals et al. instead use attention as a pointer to the input element for the output, hence the name 'Pointer Networks'. In problems where we are concerned with sets instead of sequences for the input recurrent networks such as LSTMs should not be used, but rather standard ANNs that are then passed to an LSTM that is trained repetitively to ensure that the network does not vary given different alterations of a set of inputs. The use of such networks for discrete problems is encouraging and could be used more extensively for NLP tasks.
  <h2>Graph and Tree Structure Modeling</h2>
  Many tasks in speech, vision and language can be viewed as acyclic/cyclic and undirected/directed graphs for language models, semantic similarity/relatedness/distance, topic detection, text summarization etc. Additionally, such graphs can be factored in such a way that they can be represented as trees which can have both representation and computational benefits for current models. Therefore, I am interested in how standard ANN gradient descent learning can be applied in graphs and trees (instead of recurrent networks) that make use of local information that is propagated and merged with other neighborhoods of local inputs to perform a task (similar in nature to Geoffrey Hinton's notion of capsules for vision \footnote{Capsule theory not published but expressed here: http://cseweb.ucsd.edu/~gary/cs200/s12/Hinton.pdf}).
  I proposed techniques and algorithms for search in graphs/trees, similarity/distance learning and classification.

  Micheli et al. (micheli2009neural) proposed a learning algorithm for graphs called NN4G. The NN4G is essentially a neural network architecture for graphs that uses

  <img src="images/nn4g.png" alt="NN4G">
  
</body>
  
  
</html>
