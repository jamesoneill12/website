<!DOCTYPE html>
<html lang="en">
<head>
  <title>Bootstrap Example</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link rel="stylesheet" href="style/research.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</head>

<body>
  
  <div class="header">
  <h2>Research Interests<</h2>
</div>

<div class="topnav">
  <a href="research_interests/capsule_networks.html">Capsule Networks</a>
  <a href="research_interests/adversarial_networks.html">Adversarial Networks</a>
  <a href="research_interests/reinforcement_learning.html">Reinforcement Learning</a>
  <a href="research_interests/graph_modelling" style="float:right">Graph Structure Modelling</a>
</div>
 
<div class="row">
  <div class="leftcolumn">
    <div class="card">
      <h2>Capsule Networks</h2>
      <h5>Here we discuss how Capsule Networks work and how they are different to Convolutional Neural Networks through the use 
        of a dynamic routing algorithm, Dec 28, 2017</h5>
      <a class="divLink" href="images/capsule_networks.png">
      <p>This page discusses some of my recent research ventures/interests.</p>
    </div>
    <div class="card">
      <h2>Adversarial Networks</h2>
      <h5>We will discuss basic Generative Adversarial Networks to some variations such as WassersteinGANs, Conditional GANs Bayesian GANs etc., Sep 2, 2017</h5>
      <a class="divLink" href="images/GAN.png">
      <p>Some text..</p>
    </div>
  </div>
  <div class="rightcolumn">
    <div class="card">
      <h2>About Me</h2>
      <div class="fakeimg" style="height:100px;">Image</div>
      <p>Some text about me in culpa qui officia deserunt mollit anim..</p>
    </div>
    <div class="card">
      <h3>Popular Post</h3>
      <div class="fakeimg"><p>Image</p></div>
      <div class="fakeimg"><p>Image</p></div>
      <div class="fakeimg"><p>Image</p></div>
    </div>
    <div class="card">
      <h3>Follow Me</h3>
      <p>Some text..</p>
    </div>
  </div>
</div>

<div class="footer">
  <h2>Footer</h2>
</div>
  
  
  
  <h1>Research Interests</h1>
  My current and primary research interests can be split into 5 broad categories; Sequence Modeling, Graph/Tree Modeling, Transfer Learning, Reinforcement Learning and Generative Modeling. Non-surprisingly, these categories overlap considerably e.g knowledge transfer is concerned with all other sections, while many discussed issues in sequence modeling are also present in Graph and Tree structure models.
  <h2>Deep Sequence Modeling</h2>
  Sequence modeling is a major part of many tasks in AI such as object/video recognition, speech recognition, NLP classification tasks, agent based modeling and essentially any tasks that are sequential in nature. Below contains major research facets in this field and how I wish to contribute to them to extend the existing work in each respective area.
  \subsubsection{Sequence to Sequence Modeling in NLP}
  Sequence to sequence models in NLP is commonly used for machine translation, multi-modal image captioning, question-answering systems, conversational systems, text summarization etc. Many of these tasks are still need of algorithmic improvements although some recent advances have made a distinct improvement ~\cite{bahdanau2014neural,sutskever2014sequence,bahdanau2014neural,xu2015show}
  <h3>Pairwise Learning</h3>
  Learning relationships between sequences is an important focus within deep learning, particularly in NLP. Siamese Networks ~\cite{bromley1994signature} are used to learn relationships between instances e.g sentences, images etc. Many problems require relationship learning of this type but it has not been applied extensively in fields where it can be of crucial benefit, such as NLP (with the exception of tasks such as sentence similarity). I foresee pairwise learning architectures to play a significant role for learning relationships for instances within a given dataset but also learning relationships across different datasets where the instances arise from different but related spaces $latex \mathcal{X}_{t1}$ and $latex \mathcal{X}_{t2}$. The design of appropriate optimization and loss functions that are designed specifically for relationship learning has not been explored in great detail and identifying appropriate loss functions for types of tasks (e.g contrastive loss vs mse) be a worthwhile research venture. Tangential comparisons to neuroscience research that describes how particular clusters of neurons activate based EEG and fMRI studies could potentially align, motivate and partially justify the ideas for relationship learning based on how the our brain carries out learning of related concepts.
  <h3>External Memory, Pointer Networks and Neural Turing Machines</h3>
  <strong>External memory</strong> has become an interest of mine, decoupling memory from the neural network while being fully differentiable had not been achieved up until recently. Graves (graves2014neural) introduced<strong> Neural Turing Machines</strong> (NTMs) in 2014 that achieved this, meaning an end to end memory network was designed with read and write operations to a memory matrix that attends to all memory elements. Vinyals et al. <strong>vinyals2016matching</strong> have used external memory to help with the problem of learning from new concepts with few instance, often referred to as few shot-learning.

  <strong>Pointer networks</strong> were introduced by Vinyals et al. (vinyals2015pointer) for sequence modelling tasks that require to keep track of ordering in the input for the task of ordering variable sized sequences. The idea here is to learn the probability of an output sequence of discrete tokens that correspond to positions in an input sequence, as standard recurrent networks have a difficult time keeping input order into account, a problem even the aforementioned NTMs cannot overcome. Pointer networks are essentially a variation of <em>Seq-2-Seq</em> models with attention, where attention is a softmax distribution with size equal the input length. In comparison to encoder decoder architectures where we only look back at the last encoder state whereas the pointer network can attend to different states of the encoder. Essentially, in comparison to using attention to output a weighted average of encoder states for the decoder state utilize, Vinyals et al. instead use attention as a pointer to the input element for the output, hence the name 'Pointer Networks'. In problems where we are concerned with sets instead of sequences for the input recurrent networks such as LSTMs should not be used, but rather standard ANNs that are then passed to an LSTM that is trained repetitively to ensure that the network does not vary given different alterations of a set of inputs. The use of such networks for discrete problems is encouraging and could be used more extensively for NLP tasks.
  <h2>Graph and Tree Structure Modeling</h2>
  Many tasks in speech, vision and language can be viewed as acyclic/cyclic and undirected/directed graphs for language models, semantic similarity/relatedness/distance, topic detection, text summarization etc. Additionally, such graphs can be factored in such a way that they can be represented as trees which can have both representation and computational benefits for current models. Therefore, I am interested in how standard ANN gradient descent learning can be applied in graphs and trees (instead of recurrent networks) that make use of local information that is propagated and merged with other neighborhoods of local inputs to perform a task (similar in nature to Geoffrey Hinton's notion of capsules for vision \footnote{Capsule theory not published but expressed here: http://cseweb.ucsd.edu/~gary/cs200/s12/Hinton.pdf}).
  I proposed techniques and algorithms for search in graphs/trees, similarity/distance learning and classification.

  Micheli et al. (micheli2009neural) proposed a learning algorithm for graphs called NN4G. The NN4G is essentially a neural network architecture for graphs that uses

  <img src="images/nn4g.png" alt="NN4G">

  <h2>Transfer, Multi-task &amp; Domain Adaptation Learning</h2>
  More often than not in machine learning (ML), we train models from scratch and do not take advantage of data and learnt models for similar and related tasks. More recently, the goal of consolidating knowledge across tasks has become a very active area of research (transfer learning or meta-learning). I refer to this type of learning as ‘horizontal learning’ (learning what to transfer across tasks) as opposed to ‘vertical learning’ (task specific learning).

  An interesting recent paper by Andrychowicz et al. (andrychowicz2016learning) view transfer learning as an optimization problem, proposing optimization as a learning problem and comparing against typical optimizations such as ADAM, Nesterov Accelerated Gradient (NAG) and RMSProp. This allows the optimizers to be identified that are well suited to particular types of functions.

  We witnessed a remarkable degree of transfer, with for example theLSTM optimizer trained on 12,288 parameter neural art tasks being able to generalize to tasks with 49,152 parameters, different styles, and different content images all at the same time. We observedsimilar impressive results when transferring to different architectures in the MNIST task.The results on the CIFAR image labeling task show that the LSTM optimizers outperform hand-engineered optimizers when transferring to datasets drawn from the same data distribution.
  <h2>One/Few Shot Learning</h2>
  Humans often learn from only few examples, leveraging knowledge from what they know about related concepts in the world to help guide their understanding for a new concept. I am interested in tasks where little is known about a given class or at least classes are highly imbalanced, a ubiquity in ML.

  One shot imitation learning is a recent approach presented by Duan et al. ~\cite{duan2017one} to learn from little number of examples using an imitation network

  Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations. For example,a task could be to stack all blocks on a table in to a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a sub-set of all tasks. A neural net is trained that takes as input one demonstration and the current state(which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention al-lows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an over-whelming variety of tasks
  <h2>Deep Reinforcement Learning in Natural Language State Space</h2>
  Recently, I have taken a strong interest and appreciation for decision process, control and prediction for goal-orientated agent based modeling. Hence, I have recently been considering where NLP problems can be viewed as agents navigating a language state space. Commonly, dialogue systems usually have a goal of coming to some significant resolve which can be viewed as a goal, likewise in Question-Answering system the goal is to produce the correct, or most contextually correct answer to the user.
  <h2>Generative Modeling for Text Generation</h2>
  For classification, discriminative modeling involves direct posterior estimation, the conditional probability $latex p(y|x)$. In generative modeling the joint distrbution $latex p(x,y)$ is first estimated and then prediction is carried out using Bayes rule. In this sense, generative models are not only used for prediction but also for interpolation of likely sample pairs $latex (x,y)$ from the estimated posterior.
  Ng and Jordan ~\cite{ng2002discriminative} in 2002 discuss this in their paper \textit{On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes}. They tested the hypothesis that discriminative models such as logistic regression perform better as the datasets size grows, but for initial performance on smaller batches, generative models such as Naive Bayes are favored.
  When the task involves direct estimation of $latex p(x,y)$ it makes sense to directly estimate the underlying distribution using a generative model, given that are enough instances to train on, proportional to the number of parameters in the generative model. Furthermore, generative models have the benefit of interpolating to unseen instances in $latex \mathcal{D}$ e.g generating whole new sentences in continuous space and not being bounded by the vocabulary only existing in $latex \mathcal{D}$.
  Bowman et al ~\cite{bowman2015generating} generated sentences from a continuous space using an RNN variational autoencoder language model, essentially generating whole sentence instead of word by word predictions. This true type of generative modeling better captures higher level syntactic attributes of the sentences and underlying joint distribution. Typically, coherency in new generated sentences is difficult. That is to say, what type of constraints need to be used for generative models to produce new sentences that are highly coherent but yet unique in their interpolation. This is a potential research question I would be interested in pursuing.

  Additionally, generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) have seen great success recently, primarily on vision tasks. Although, only very recently have generative models made an impact on NLP text generation. Hu et al.~\cite{hu2017toward} have proposed to use VAEs that use the wake sleep algorithm to generate fake samples of short sentences during the sleep phase that are then fed back in the training process during the wake phase. The model is based off ~\cite{bowman2015generating}

  \textbf{Applying discriminators to text generation is hard due to the non-differentiability of discrete samples (Yu et al., 2017; Zhang et al., 2016; Kusner \&amp; Hernndez-Lobato, 2016). Bowman et al. (2015); Tang et al. (2016); Yang et al. (2017) instead use VAEs without discriminators. All these text generation methods do not learn disentangled latent representations, resulting in randomized and uncontrollable samples. In contrast, disentangled
  generation in visual domain has made impressive progress. E.g., InfoGAN (Chen et al., 2016), which resembles the extended sleep procedure of our joint VAE/wakesleep algorithm, disentangles latent representation in an unsupervised manner. The semantic of each dimension is observed after training rather than designated by users in a controlled way. Siddharth et al. (2017); Kingma et al.
  (2014) base on VAEs and obtain disentangled image representations with semi-supervised learning. Zhou \&amp; NeuToward Controlled Generation of Text big (2017) extend semi-supervised VAEs for text transduction. In contrast, our model combines VAEs with discriminators which provide a better, holistic metric compared to element-wise reconstruction. Moreover, most of these approaches have only focused on the disentanglement of the structured part of latent representations, while ignoring potential dependence of the structured code with attributes not explicitly encoded. We address this by introducing an independency constraint, and show its effectiveness for improved interpretability.}

  Also, GANs in particular are notoriously difficult to train, which arises the question, <em>What controls should be used to ensure stability and convergence when training GANs for in continuous embedding space</em> $latex \mathcal{X}$ ?. Hu et al.'s paper has made a start on answering this for text generation with VAEs but this is only the beginning of this kind of work where many improvements are still to be made to generate human level novel coherent sentences.

  &nbsp;
  
</body>
  
  
</html>
